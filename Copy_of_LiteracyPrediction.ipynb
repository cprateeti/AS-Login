{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LiteracyPrediction.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cprateeti/Account/blob/master/Copy_of_LiteracyPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIfh2REWFr6p",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrIlNFZeKMsx",
        "colab_type": "text"
      },
      "source": [
        "At the end of this experiment, you will be able to:\n",
        "\n",
        "* Perform Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5DokZ7MF1P_",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3vgcWwOF2cK",
        "colab_type": "text"
      },
      "source": [
        "### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqJ1h77-KMsy",
        "colab_type": "text"
      },
      "source": [
        "We will be using district wise demographics, enrollments, school and teacher indicator data to predict whether the literacy rate is high / medium / low in each district."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md2IjdMdGCWm",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B5ztQVbKMsz",
        "colab_type": "text"
      },
      "source": [
        "Data preprocessing is an important step of solving every machine learning problem. Most of\n",
        "the datasets used with Machine Learning problems need to be processed / cleaned / transformed\n",
        "so that a Machine Learning algorithm can be trained on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsxaJLZAKMs0",
        "colab_type": "text"
      },
      "source": [
        "There are different steps involved for Data Preprocessing. These steps are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF3Eg-5pKMs1",
        "colab_type": "text"
      },
      "source": [
        "    1. Data Cleaning → In this step the primary focus is on\n",
        "        -Handling missing data\n",
        "        -Handling nosiy data\n",
        "        -Detection and removal of outliers\n",
        "    \n",
        "    2. Data Integration → This process is used when data is gathered from various data sources\n",
        "    and data are combined to form consistent data. This data after performing cleaning is used\n",
        "    for analysis.\n",
        "    \n",
        "    3. Data Transformation → In this step we will convert the raw data into a specified for-\n",
        "    mat according to the need of the model we are building. There are many options used for\n",
        "    transforming the data as below:\n",
        "        -Normalization\n",
        "        -Aggregation\n",
        "        -Generalization\n",
        "        \n",
        "    4. Data Reduction → After data transformation and scaling the redundancy within the data\n",
        "    is removed and efficiently organizing the data is performed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd9cGZeydAyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to download the data (you will get the zip file)\n",
        "!wget https://cdn.talentsprint.com/aiml/Experiment_related_data/data-20190108T113429Z-001.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0OLk-Y-8yfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to unzip the data\n",
        "!unzip data-20190108T113429Z-001.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7VD8dJgGhVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9utRh_WiPNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDWnvJ6QiRB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZSlj_nWKMs4",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 1 \n",
        "We have four different files\n",
        "\n",
        "* Districtwise_Basicdata.csv\n",
        "* Districtwise_Enrollment_details_indicator.csv\n",
        "* Districtwise_SchoolData.csv\n",
        "* Districtwise_Teacher_indicator.csv\n",
        "These files contain the neccesary data to solve the problem.\n",
        "Load all the files correctly, after observing the header level details, data records etc\n",
        "\n",
        "Hint : Use read_csv from pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XRregSb9wdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "basic=pd.read_csv('Districtwise_Basicdata.csv')\n",
        "enrolment=pd.read_csv('Districtwise_Enrollment_details_indicator.csv')\n",
        "school=pd.read_csv('Districtwise_SchoolData.csv')\n",
        "teacher=pd.read_csv('Districtwise_Teacher_indicator.csv')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7v4yLCwKMs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Here\n",
        "!cat Districtwise_Enrollment_details_indicator.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2A3OKraKMs9",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 2  \n",
        "\n",
        "* Remove the unwanted columns, which are unlikely to contribute for the prediction of overall literacy grade. The decision of what constitutes unwanted columns depends on how it effects your final accuracy (and very little on your domain understanding of education sector in India; you're encouraged however to exercise some domain understanding too if you wish)\n",
        "\n",
        "**Hint** use pandas drop function to drop your choice of unwanted columns (if any).\n",
        "\n",
        "\n",
        "* As the required data is present in different files, we need to integrate all the four to make single dataframe/dataset. For that purpose, create a unique identifier for each row in all the dataframes so that it can be used to map the data in different files correctly\n",
        "* Join/integrate this data \n",
        "\n",
        "Example : data of the district ananthapur in Andrapradesh, which present in different files should form a single row \n",
        "\n",
        "Hint : \n",
        "* Use the combination of year, statecode, district code as unique identifier \n",
        "\n",
        "* Refer the following link for merge, join and concat syntaxes:  \n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/merging.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le5wGzUuKMs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Here\n",
        "import pandas as pd\n",
        "basic=pd.read_csv('Districtwise_Basicdata.csv',header=[1])\n",
        "basic['id']=basic.groupby(['Year','Statecd','distcd']).ngroup()\n",
        "basic_sep=basic.iloc[:,[0,1,3,9,17,19]]\n",
        "basic_sep\n",
        "#basic.columns.get_loc(\"Total Poulation\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9whxYbsj3lvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "enrolment=pd.read_csv('Districtwise_Enrollment_details_indicator.csv',header=[3])\n",
        "enrolment['id']=enrolment.groupby(['Year','Statecd','distcd']).ngroup()\n",
        "enrolment\n",
        "#data\n",
        "#data=pd.concat([basic_sep,enrolment,school,teacher],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mQPAiQrclPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "school=pd.read_csv('Districtwise_SchoolData.csv',header=[3])\n",
        "school['id']=school.groupby(['ac_year','Statecd','distcd']).ngroup()\n",
        "school\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ykmmalcrVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "teacher=pd.read_csv('Districtwise_Teacher_indicator.csv',header=[3])\n",
        "teacher['id']=teacher.groupby(['ac_year','statecd','distcd']).ngroup()\n",
        "teacher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNYHHS3qVrpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.merge(basic_sep,enrolment,on='id',how='inner')\n",
        "data1=pd.merge(school,teacher,on='id',how='inner')\n",
        "data3=pd.merge(data,data1,on='id',how='inner')\n",
        "data3.rename(columns={\"Year_x\":\"Year\",\"Year_y\":\"Year\",\"Statecd_x\":\"statecd\",\"Statecd_y\":\"statecd\",\"distcd_x_x\":\"distcd\",\"distcd_y_x\":\"distcd\",\"State Name _x\":\"Statename\",\"State Name _y\":\"Statename\",\"distname_x\":\"distname\",\"ac_year_x\":\"Year\",\"statename\":\"Statename\",\"distname_y\":\"distname\",\"ac_year_y\":\"Year\",\"acyear\":\"Year\"},inplace=True)\n",
        "data3=data3.loc[:,~data3.columns.duplicated()]\n",
        "data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3wf8dFrKMtC",
        "colab_type": "text"
      },
      "source": [
        "Follow this steps in order to clean the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jcX4aRsKMtE",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 3 \n",
        "\n",
        "* Overall_lit is our target variable, which we need to predict. Delete the row with missing overall_lit column\n",
        "* Take a call to replace the missing values in any other column appropriately with mean/median/mode\n",
        "* Convert categorical values to numerical values\n",
        "Example : If a feature contains categorical values such as dog, cat, mouse etc then replace them with 1, 2, 3 etc or using one hot encoding (your judgement)\n",
        "\n",
        "*Hint* :\n",
        "* Use pandas fillna function to replace the missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REcYtu79KMtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbE-SB6iva02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import numpy as np\n",
        "#data3 = data3.replace(0, np.nan)\n",
        "#data3 = data3.dropna(how='all', axis=0)\n",
        "#data3 = data3.replace(np.nan, 0)\n",
        "#data3 = data3[data3['overall_lit'].notna()]\n",
        "#data3.dropna(inplace=True)\n",
        "#data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKwAVYolEMEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data3 = data3[data3['overall_lit'].notna()]\n",
        "#data3.dropna(inplace=True)\n",
        "#data3.drop(['Statename','distname'],axis=1,inplace=True)\n",
        "#data3.replace({\"High\":3,\"Low\":1,\"Medium\":2},inplace=True) \n",
        "#data3 = data3.loc[:, (data3 != 0).any(axis=0)]\n",
        "#mean1=data3['Enr Govt7'].mean()\n",
        "#data3['Enr Govt7']= data3['Enr Govt7'].replace(0,mean1)\n",
        "#data3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9w1nc4jaLrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "data3 = data3.replace(0, np.nan)\n",
        "data3 = data3.dropna(how='all', axis=1)\n",
        "data3 = data3.replace(np.nan, 0)\n",
        "data3 = data3[data3['overall_lit'].notna()]\n",
        "data3.drop(['Statename','distname','Year','statecd','distcd'],axis=1,inplace=True)\n",
        "data3.replace({\"High\":3,\"Low\":1,\"Medium\":2},inplace=True) \n",
        "mean1=data3.loc[:,:].mean()\n",
        "data3.loc[:,:]= data3.loc[:,:].replace(0.0,mean1)\n",
        "#data3.select_dtypes(include='object')\n",
        "data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MwCq3SrI8ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target=data3['overall_lit']\n",
        "target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzsY-knQKMtI",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 4 \n",
        "\n",
        "Use the functions below to adjust the outliers\n",
        "\n",
        "smooth_out function takes pandas dataframe as input and caculates mean, standard deviation of every column to check whether all the values in that lies within the range of mean +/- 2*standard_deviation of that column or not.\n",
        "If any of the values are not present in that boundary, then that values is brought on to the boundary.\n",
        "\n",
        "**Hint:** Should  the index column be normalized too? \n",
        "\n",
        "<img src=\"https://cdn.talentsprint.com/aiml/Experiment_related_data/normal_dist.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhdy63OSKMtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to clip and clam the data\n",
        "def clip_clamp(x, mean, sd):\n",
        "    # Checking whether the value is less than a differenced value between mean and standard deviation.\n",
        "    if x < mean - 2*sd :\n",
        "        return mean - 2*sd\n",
        "    #Checking whether the value is greater than a differenced value between mean and standard deviation.\n",
        "    elif x > mean + 2*sd :\n",
        "        return mean + 2*sd\n",
        "    # If above two conditions are not statisfied we will return the original value\n",
        "    else :\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXWSJ2wCKMtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to smooth the data\n",
        "def smooth_out(Total_data):\n",
        "    for i in Total_data.columns:\n",
        "        # Calculating the mean value\n",
        "        mean = np.mean(Total_data[i].values, axis=0)\n",
        "        # Calculating the standard deviation value\n",
        "        sd = np.std(Total_data[i].values, axis=0)\n",
        "        # Calculating the corrected value using clip and clamp function\n",
        "        corrected = np.array([clip_clamp(x, mean, sd) for x in Total_data[i].values])\n",
        "        # Storing the data in form of series\n",
        "        Total_data[i] = pd.Series(corrected, index=Total_data[i].index)\n",
        "    return Total_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hU2poUeKMtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Here\n",
        "data3=smooth_out(data3)\n",
        "data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsxP4SY4BgWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.regplot(x=\"overall_lit\",y=\"totpopulation\",data=data3)\n",
        "plt.ylim(0,)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dMbvU-KKMtY",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 5 \n",
        "\n",
        "Use the function below (corr_features) to identify uncorrelated features and remove the remaining features\n",
        "* corr_features takes pandas dataframe, columns in the dataframe and bar (corelation co-efficient)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgJeGouOKMtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to find uncorrelated features\n",
        "def corr_features(df,cols,bar=0.9):\n",
        "    for c,i in enumerate(cols[:-1]):\n",
        "        col_set = set(cols)\n",
        "        for j in cols[c+1:]:\n",
        "            if i==j:\n",
        "                continue\n",
        "           \n",
        "            score = df[i].corr(df[j])\n",
        "            \n",
        "            if score>bar:\n",
        "                cols = list(col_set-set([j]))\n",
        "            if score<-bar:\n",
        "                cols = list(col_set-set([j]))\n",
        "    return cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gisomp5aa9Qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Here\n",
        "corr_columns = corr_features(data3,data3.columns)\n",
        "data3=data3.loc[:,corr_columns]\n",
        "data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5njOPWIXKMtd",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 6 \n",
        "\n",
        "Perform Mean Correction and Standard Scaling on the data feature/column wise.\n",
        "\n",
        "**Hint:** In order to understand the idea behind the terms used above, you may refer the following link: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "988wdlpDKMtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Here\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "final_data = scalar.fit_transform(data3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmPgmB2THXV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(final_data.mean())\n",
        "print(final_data.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QugqX021KMtl",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 7 **(Optional)**\n",
        "\n",
        "you can apply different classifiers(from sklearn) on the preprocessed data ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6ka-fmz8gGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def callKnn(data,targets):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.33)\n",
        "    neigh = KNeighborsClassifier(n_neighbors=7)\n",
        "    neigh.fit(X_train, y_train)\n",
        "    predicted_labels = neigh.predict(X_test)\n",
        "    return accuracy_score(y_test,predicted_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEjD56Puavy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Here\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "callKnn(final_data,target)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}